\chapter{2/22/2023: Even More Martingales}
\section{More Martingale Computations}
Recall the drunk monkey problem from the end of last class. We can solve similar problems such as the number of (fair) coin flips expected to flip the following series \begin{enumerate}
	\item HHH ($2^3+2^2+2^1=14$)
	\item HTH ($2^3+2^1=10$)
	\item HTT ($2^3=8$)
\end{enumerate}
\nt{
	\textit{This isn't important but I thought it was interesting; feel free to skip this note.} \\[0.1in]
	Upon first glance, it may seem fairly unintuitive that \textit{more} repetition in the desired sequence is associated with a \textit{later} expected stopping time. After all, the sequence HHH has a lot of symmetry, and any individual heads toss can show up in the stopping sequence in three different places. Shouldn't this mean that the sequence HHH is ``easiest'' to obtain and thus have the lowest expected stopping time? \\[0.1in]
	I think this logic fails because of the sequential nature with which these games are played. In particular, when we report an individual flip, that result is not in isolation; it just becomes the last result in a sequence of flips that have already been decided. Thus, if the previous flip was tails and the new flip is heads, then this heads can only be in the first position of the HHH goal sequence (if it is part of that sequence at all). Similarly, if the previous two flips were tails then heads, then flipping a heads could only put it in the second position of the goal HHH sequence (again, \textit{if} it ends up being part of that sequence at all). Thus, the symmetry that seems like it should make sequences such as HHH easier to obtain actually does not have a helpful effect at all. \\[0.1in]
	This explains why the expected stopping time for the ``symmetric'' sequences shouldn't be lower than those of the ``assymmetric'' sequences, but as we have calculated, the ``symmetric'' sequences stopping times are in fact \textit{higher} -- why is this the case? The answer seems to be that the less ``symmetric'' sequences are often less punished than mistakes in the ``symmetric'' sequences. Consider the simple examples of flipping a fair coin until we reach the sequence HH. If the last flip was a head (and the game has not ended), then there are two possibilities: either we flip a head and are done, or flip a tails and have to start completely over again as none of the previous flips will be able to form the HH sequence necessary to end the game. Suppose we instead flip until we reach the sequence HT. Then, when the last flip was an H (and again the game has not ended), then there are similarly two possibilities for the next flip: either we flip a tails and are done, or flip a heads. However, in this case, when we flip a heads, we don't have to start completely over; this last heads can be used as the starting point for the HT ending sequence that we need, and so the expected number of flips to reach HT should be less than the same quantity for HH, which agrees with our calculations. This general trend extends to more complicated scenarios, such as trying to type ABRACADABRA.
}

\section{Importance of the Amount of Information Available}
Suppose we have $P_{t+1}=P_t+X_t+\varepsilon_{t+1}$ where $\mathbb E[X_t]=0$, $\mathbb E[\varepsilon_{t+1}]=0$, $X_t$ is independent of $P_t$, and $\varepsilon_{t+1}$ is some random noise variable independent of the other variables. Furthermore let $\mathcal F_t$ and $\mathcal G_t$ be the information generated by the sequences $\{P_k\}_{k=1}^t$ and $\{(P_k,X_k)\}_{k=1}^t$, respectively. Then, we can see that \begin{align*}
	\mathbb E[P_{t+1}|\mathcal F_t] &= \mathbb E[P_t+X_t+\varepsilon_{t+1}|\mathcal F_t] \\
									&= P_t + \mathbb E[X_t|\mathcal F_t] + 0 \\
									&= P_t.
\end{align*} Thus, $(P_t,\mathcal F_t)$ is a Martingale. However, note that $$\mathbb E[P_{t+1}|\mathcal G_t] = P_t + \mathbb E[X_t|\mathcal G_t] + 0 = P_t + X_t,$$ and thus $(P_t,\mathcal G_t)$ is not a Martingale. This is essentially insider trading -- if we have access to $\mathcal G_t\supset \mathcal F_t$ which has all the information about $X_t$, then we can predict price movement better than anyone else.

\section{More Properties of Stopping Times}
Before proving Doob's OST, we explore another important theorem about stopping times.
\theorem{Wald's Theorem}{If $X_i$ are i.i.d. with finite expectation $\mathbb E[|X_i|]<\infty$ for all $i$, and if $N$ is the stopping time also with $\mathbb E[N]<\infty$, then $$ \mathbb E\left[\sum_{n=1}^N X_n\right] = \mathbb E[N]\mathbb E[X_i]. $$} This theorem is really nice because it can apply to such a wide variety of stopping times with the only limitation being that the expectation of this stopping time must be finite.
\pf{We prove this essentially by direct computation. Letting $\mathcal F_n$ be the information contained in $\{X_n\}_{n=1}^N$: \begin{align*}
		\mathbb E\left[\sum_{n=1}^{N} X_n\right] &= \mathbb E\left[\sum_{n=1}^\infty X_n\cdot\mathbf 1_{N\geq n}\right] \\
												 &= \sum_{n=1}^\infty \mathbb E \left[ \mathbb E \left[ X_n\cdot\mathbf 1_{N\geq n} \bigg\rvert\mathcal F_{n-1} \right]  \right] \\
                         &= \sum_{n=1}^{\infty} \mathbb E \left[ \mathbf 1_{N\geq n}\mathbb E[X_n|\mathcal F_{n-1}] \right] .
  \end{align*} Note that the last step is valid because given $\mathcal F_{n-1}$, $\mathbf 1_{N\geq n}$ is simply a constant (since we know if the stopping condition has happened before $n$). Then, this sum becomes $$ \sum_{n=1}^{\infty} \mathbb E \left[ \mathbf 1_{N\geq n}\mathbb E[X_n|\mathcal F_{n-1}] \right] = \sum_{n=1}^{\infty} \mathbb E[\mathbf 1_{N\geq n}]\cdot \mathbb E[X_i] = \left(\sum_{n=1}^\infty \mathbb P[N\geq n]\right)\mathbb E[X_i] = \mathbb E[N]\mathbb E[X_i] $$ 
}
\nt{To see that $\sum_{n=1}^\infty \mathbb P[N\geq n]=\mathbb E[N]$ in the last equality of the proof above:
\begin{center}
\begin{tabular}{ccccccccccc}
	& $\mathbb P[N\geq 1]$ & $=$ & $\mathbb P[N=1]$ & $+$ & $\mathbb P[N=2]$ & $+$ & $\mathbb P[N=3]$ & $+$ & $\cdots$ \\
	& $\mathbb P[N\geq 2]$ & $=$ & & & $\mathbb P[N=2]$ & $+$ & $\mathbb P[N=3]$ & $+$ & $\cdots$ \\
	& $\mathbb P[N\geq 3]$ & $=$ & & & & & $\mathbb P[N=3]$ & $+$ & $\cdots$ \\
	$+$ & $\vdots$ & & $\vdots$ & & $\vdots$ & & $\vdots$ & &  \\ \hline
	& $\sum_{n=1}^\infty \mathbb P[N\geq n]$ & $=$ & $\mathbb P[N=1]$ & $+$ & $2\cdot\mathbb P[N=2]$ & $+$ & $3\cdot\mathbb P[N=3]$ & $+$ & $\cdots$ \\
	& & $=$ & $\mathbb E[N]$ & & & & & &
\end{tabular} 
\end{center} 
}

Proving Doob's Theorem is pretty tricky, so we will go part of the way there by proving what is essentially a precursor to it.
\theorem{Doob's OST Precursor}{For any stopping time $\tau$, let $\tau'=\min(\tau,n)=\tau\wedge n$. Then, for the Martingale $\{M_n,\mathcal F_n\}$, $$\mathbb E[M_{\tau\wedge n}] = \mathbb E[M_0].$$ }

Note that Doob's OST essentially comes by taking the limit as $n$ goes to $\infty$. This isn't super straightforward (can't always switch limit and expectation), but it works out nicely in this case.

To prove this theorem, we will make use of the following Lemma:
\lemma{}{
Without loss of generality, let $M_0=0$. Then, if $$Z_n = \sum_{k=1}^{n} A_k(M_k-M_{k-1}), $$ then $(Z_n,\mathcal F_n)$ is a Martingale.
}

This lemma will be proved in the homework (presumably PSet 3). We now use this lemma to prove the precursor to Doob's OST.
\pf{Note that \begin{align*}
	M_{\tau\wedge n} &= M_\tau\cdot\mathbf 1_{\tau\leq n-1} + M_n\cdot\mathbf 1_{\tau\geq n} \\
					 &= M_0 + \sum_{k=1}^n A_k(M_k-M_{k-1})
\end{align*} where $A_k=\mathbf 1_{\tau\geq k}$. The last equality can be seen through some simple expansion and telescoping. Then the sum in the last expression is a Martingale that initially has an unconditional expected value of 0, and thus taking unconditional expectations of both sides completes the proof. (Note that the $M_0$ was not on the board when this was explained in class, but I'm pretty sure we need it; I think it was just assumed to be 0 as in the lemma.)
}

