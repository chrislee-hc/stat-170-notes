\chapter{2/13/2023: Martingales}
\section{Some Background}
First recall Adam's Law, or the Law of Total Expectation, which states that $\mathbb E[\mathbb E[X|Y]] = \mathbb E[X]$. Martingales are based on this concept in a slightly different language.

We will be thinking about conditional expectation using the framework of \textit{information}. Define $\mathcal F_Y$ to be the information contained in $Y$. Then we can write $\mathbb E[X|\mathcal F_X] = X$; this is essentially the more formal (``mathematically correct``) way to write $\mathbb E[X|X] = X$. Then we can rewrite Adam's Law as $$ \mathbb E[\mathbb E[X|\mathcal F_Y]] = \mathbb E[X]. $$ Similar to the logic behind Adam's Law, we can also have $\mathbb E[\mathbb E[X|Y,Z]|Z] = \mathbb E[X|Z]$. Written in the information framework, we have $$\mathbb E[\mathbb E[X|\mathcal F_Y]|\mathcal F_Z]=\mathbb E[X|\mathcal F_Z]$$ given that $\mathcal F_Y\supseteq\mathcal F_Z$, meaning that $Y$ contains at least as much information as $Z$ (for example, if $Y=X^2$, then $\mathcal F_X\supseteq\mathcal F_Y$).

Also recall Markov Chains, which have the Markov property $$ \mathbb P[X_{t+1}|X_t,X_{t-1},\cdots,X_1] = \mathbb P[X_{t+1}|X_t]. $$ 

\section{Martingale Definition}
\dfn{Martingale}{
	$(M_t,\mathcal F_t)$ is a \textbf{martingale} if 
	\begin{itemize}
		\item $\mathcal F_t\supseteq\mathcal F_{t-1}$ where $\mathcal F_t=\operatorname{Inf}(M_1,\cdots,M_t)$ is the information contained in the previous $M_i$ values; this means that we have at least as much information at time $t$ as we do at time $t-1$
		\item $\mathbb E[M_t|\mathcal F_{t-1}]=M_{t-1}$ for all $t$
		\item $\mathbb E[|M_T|]<\infty$
	\end{itemize}
}
Concerning the last point, variance can be infinite, but infinite expected values would break everything. Also note that the key difference from Markov Chains is that Martingales make no claims about distributions and instead only care about means.

\ex{}{
Suppose that $X_i$ are i.i.d. with $\mathbb E[X_i]=0$, and define \begin{align*}
	M_0 &= 0 \\
	M_n &= \sum_{k=1}^n X_k \\
	\mathcal F_n &= \operatorname{Inf}(X_1,\cdots,X_n).
\end{align*} Then $(M_n,\mathcal F_n)$ is a martingale since we have $$ \mathbb E[M_n|\mathcal F_{n-1}] = \mathbb E[X_n + M_{n-1}|\mathcal F_{n-1}] = \mathbb E[X_n|\mathcal F_{n-1}] + \mathbb E[M_{n-1}|F_{n-1}] = \mathbb E[X_n] + M_{n-1} = M_{n-1}. $$}

\theorem{}{
	If $m<n$, then $\mathbb E[M_n|\mathcal F_m]=M_m$.
}
\begin{proof}
	By the generalization of Adam's law from before, \begin{align*}
		\mathbb E[M_n|\mathcal F_{n-2}] &= \mathbb E[\mathbb E[M_n|\mathcal F_{n-1}]|\mathcal F_{n-2}] \\
										&= \mathbb E[M_{n-1}|\mathcal F_{n-2}] \\
										&= M_{n-2}.
	\end{align*} Then it's pretty clear we can formalize this iterative argument using induction.
\end{proof} 
\corollary{}{
	$\mathbb E[M_n] = \mathbb E[M_m]$.
}
\proof{
	Take unconditional expectations on both sides of the previous theorem and use Adam's Law.
}

Consider another example of a security that pays off $M_T$ at end time $T$. Then at times $t<T$, our expected end payoff is $P_t=\mathbb E[M_T|\mathcal F_t]$. We claim that $(P_t,\mathcal F_t)$ is a Martingale. The proof is by definition and generalized Adam's law: $$ P_t = \mathbb E[M_T|\mathcal F_t] = \mathbb E[\mathbb E[M_T|\mathcal F_{t+1}]|\mathcal F_t] = \mathbb E[P_{t+1}|\mathcal F_t]. $$ 

Notably, for some Martingale $(P_t,\mathcal F_t)$, then the expectation of the returns is 0: \begin{align*}
\mathbb E \left[ \frac{P_{t+1}-P_t}{P_t}\bigg\rvert\mathcal F_t \right]  &= \mathbb E \left[ \frac{P_{t+1}}{P_t}\bigg\rvert\mathcal F_t \right] - 1 \\
																		 &= \frac{1}{P_t}\mathbb E[P_{t+1}|\mathcal F_t] - 1 \\
																		 &= \frac{1}{P_t}\cdot P_t - 1 \\
																		 &= 0.
\end{align*} 

\dfn{Martingale Difference}{Let $(Y_t,\mathcal F_t)$ be a Martingale. Then $\varepsilon_t=Y_t-Y_{t-1}$ is a \textbf{Martingale Difference}, sometimes denoted as $\varepsilon_t = \Delta Y_t$. This is essentially the discrete first derivative of the sequence.}

We can also rewrite this definition to get \begin{align*}
	Y_{t+1} &= Y_t + \varepsilon_{t+1} \\ 
			&= Y_{t-1} + \varepsilon_t + \varepsilon_{t+1} \\
			&= \cdots
\end{align*} 

The core properties of a Martingale and its differences are: \begin{enumerate}
	\item $\mathbb E[Y_{t+s}|\mathcal F_t] = Y_t\qquad\forall s\geq 0$
	\item $\mathbb E[\varepsilon_{t+s}|\mathcal F_t] = 0\qquad\forall s\geq 0$
	\item $\operatorname{Corr}(\varepsilon_{t+i},\varepsilon_{t+j}|\mathcal F_t)=0\qquad i,j\geq 0, i\neq j$
	\item $\Var(Y_{t+s}-Y_t|\mathcal F_t) = \sum_{j=1}^{s} \Var(\varepsilon_{t+j}|\mathcal F_t). $
\end{enumerate} Note that conditions 2 and 3 are useful when doing a regression (probably some sort of autoregressive model predicting $Y_{t+1}=\beta Y_t+\varepsilon_{t+1}$. All four of these properties are also true unconditionally: \begin{enumerate}
	\item $\mathbb E[Y_{t+s}] = \mathbb E[Y_t]$
	\item $\mathbb E[\varepsilon_{t+s}] = 0$
	\item $\operatorname{Corr}(\varepsilon_{t+i},\varepsilon_{t+j}) = 0$.
	\item $\Var(Y_{t+s}-Y_t) = \sum_{j=1}^{s} \Var(\varepsilon_{t+j}). $
\end{enumerate} 
Also to see the relationship between some of these properties, we can use property 3 to see that \begin{align*}
	\Var(Y_{t+s}-Y_t|\mathcal F_t) &= \sum_{j=1}^{s} \sum_{i=1}^{s} \Cov(\varepsilon_{t+j},\varepsilon_{t+i}|\mathcal F_t)   \\
								   &= \sum_{i=1}^{s} \Var(\varepsilon_{t+j}|\mathcal F_t) + 2 \sum_{i\neq j}^{} \Cov(\varepsilon_{t+i},\varepsilon_{t+j}|\mathcal F_t) \\
								   &= \sum_{j=1}^{s} \Var(\varepsilon_{t+j}|\mathcal F_t), 
\end{align*} which gives property 4.

