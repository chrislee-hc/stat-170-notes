\chapter{3/29/2023: Time Series}
\dfn{Time Series}{A \textbf{time series} is a series of data points indexed in time order.}
Time series have tons of applications, from accounting and finance to medical sciences and many others.

\section{Stationarity}
Heuristically, a time series is stationary if the manner in which time series data changes is constant in time, without any trends or seasonal patterns. It's an important model assumption for many time series models (e.g. the ARMA model we'll look at later), so we generally want to make sure our data is stationary before applying the models.
\dfn{Stationarity}{A time series is considered \textbf{stationary} if \begin{itemize}
	\item Its mean remains constant with time.
	\item Its standard deviation remains constant with time.
	\item There is no seasonality.
\end{itemize}}
One way we can check this model assumption is by computing the rolling mean and standard deviations. For a stationary time series, we expect both of these to be roughly constant and not display increasing, decreasing, or seasonal trends.
\ex{Brownian Motion}{The Brownian motion object $\{B_t\}$ is not stationary because $B_t\sim\mathcal N(0,t)$ because its standard deviation is not constant with time. However, the \textit{increments} are stationary.}

One way we can check for stationarity is through Augmented Dickey-Fuller (ADF). One interesting thing about this test is that the null hypothesis $H_0$ is that the series is not stationary.

\subsection{Transforming a Time Series to be Stationary}
Some common methods of transforming a time series to make it stationary include taking the logarithm or taking differences (or even second, third, etc. differences).

\section{Time Series Models}
If we are using time series data to predict the future, then we might want to fit a regression such as $$ y_t = \alpha + \beta y_{t-1} + \varepsilon_t. $$ We are looking at data points $(y_{t-1},y_t)$ for all $t$ in the time interval and fitting the regression on those points. This is called an \textbf{autoregressive (AR) model} because we are regressing the data on itself. This example is specifically an example of an $\operatorname{AR}(1)$ model since we are looking just one data point in the past; generally, an $\operatorname{AR}(p)$ looks $p$ time steps in the past: $$ y_t = \beta_0 + \beta_1 y_{t-1} + \beta_2 y_{t-2} + \cdots + \beta_p y_{t-p} + \varepsilon_t. $$ Note that we are explicitly assuming that the data is autocorrelated.

If we have the data generated by an autocorrelated process; how do we figure out the value of $p$? We can use something called the partial autocorrelation function (PACF). For example, if we have $p=1$, then this method separates the correlation between $Y_t$ and $Y_{t-2}$ that is already explained by $Y_{t-1}$.

A \textbf{Moving Average (MA) model} assumes that the current value is the series mean plus a linear combination of errors from previous prediction values: $$ X_t = \mu + \sum_{i=1}^q \theta_i\varepsilon_{t-i} + \varepsilon_t. $$ The $\theta_i$ values are parameters of the model. We denote this model as $\operatorname{MA}(q)$, where $q$ is the number of time steps we look back. We can use the autocorrelation function (ACF) to identify $q$ from time series data.

