\chapter{3/6/2023: Tails and Sampling}
\section{Heavy Tailedness}
The \textbf{Cauchy distribution}, which is the $t$-distribution with one degree of freedom, is essentially the poster child of empty tailedness. The PDF of the Cauchy distribution is $$f_X(x) = \frac{1}{\pi}\cdot\frac{1}{x^2+1}$$ for all $x\in\mathbb R$. This distribution has a nonexistent mean, variance -- this implies that all moments do not exist (not trivial). We can also generate the Cauchy distribution by taking $\tan\theta$ if $\theta\sim\operatorname{Unif}\left(-\frac{\pi}{2},\frac{\pi}{2}\right)$. The geometric interpretation is that if we have a sphere of diameter 1 and a plane that it is tangent to, then a line that makes an angle of $\theta$ with the normal to the plane will intersect the plane at a distance of $\tan\theta$ from the point of tangency. Issues like this can arise especially when taking ratios, for example the ratio of the returns of a stock to the returns of an index. The fact that the mean is undefined means that we cannot rely on LLN or CLT to ensure that our estimates for mean will converge to some value.

Note that the $t$-distribution with $\nu>1$ degrees of freedom has defined moments and is less heavy tailed than a Cauchy but is heavier tailed than a Gaussian.

\subsection{Extreme Values}
One other way to think about heavy tailedness through a running maximum. Suppose we have the standard normal variables $Z_1,Z_2,\cdots,Z_n\overset{i.i.d.}{\sim}\mathcal N(0,1)$ and let $\psi_n=\max_{1\leq i\leq n} Z_i$. As $n\to\infty$, $\psi_n\to\infty$. Note that this is true for any distribution that has support that has an infinite supremum. In particular, we have something along the lines of $\psi_n\in\mathcal O(\sqrt{2\log n})$ which comes from the $\exp\left\{-\frac{x^2}{2}\right\}$ in the PDF of the standard normal.

Now if we instead consider $Y_1,\cdots,Y_n\overset{i.i.d.}{\sim}\operatorname{Cauchy}$ and let $\mu_n=\max_{1\leq i\leq n} Y_i$, then we get something along the lines of $\mu_n\sim\mathcal O(n^\alpha)$ for $\alpha=1/2$ (verify this), which is significantly faster than the rate of growth of $\psi_n$.

\subsection{Estimators}
If we have $X_1,\cdots,X_n\overset{i.i.d.}{\sim}\mathcal N(0,1)$, then $\frac{1}{n}\sum_{i=1}^n X_i=\overline X\sim\mathcal N\left(0,\frac{1}{n}\right)$. The real life implication is that as our sample size increases, the variance of our estimate of our mean decreases and eventually converges to the true value. However, this nice consequence is not nearly as nice for the Cauchy distribution.

\clm{If $Z_1,\cdots,Z_n\overset{i.i.d.}{\sim}\operatorname{Cauchy}$, then $\frac{1}{n}\sum_{i=1}^n Z_i=\overline Z\sim\operatorname{Cauchy}$. This means that when we have undefined moments, taking the average does not reduce our uncertainty at all; the tails of the estimate of the mean is just as heavy as the tails of the individual data points.}{}{}
\pf{The moment generating function of the Cauchy distribution does not exist since the moments themselves do not exist, so instead we will analyze the characteristic function $\varphi_z(t)$ which has a very similar function: $$\varphi_z(t) = \mathbb E\left[e^{itz}\right].$$ Notably, the characteristic function of the Cauchy distribution is $\varphi_z(t)=e^{-|t|}$. In particular, there is a bijection between the set of distributions and and the set of characteric functions. If we take the mean of our Cauchy variables, then \begin{align*}
    \mathbb E\left[e^{it\overline z}\right] &= \mathbb E\left[\exp\left\{it\cdot\frac{1}{n}\sum_{i=1}^n Z_i\right\}\right] \\
                                            &= \prod_{i=1}^n \mathbb E\left[ \exp\left\{i\cdot\frac{t}{n}\cdot z_n\right\} \right] \\
                                            &= \prod_{i=1}^n \exp\left\{-\left\lvert\frac{t}{n}\right\rvert\right\} \\
                                            &= e^{-|t|}.
\end{align*} This is exactly the characteristic function of the Cauchy distribution, so we are done.}
\nt{This isn't even as bad as it gets -- if $Y\sim\mathcal N(0,1)$, then estimating the mean of $\frac{1}{Y^2}$ by averaging the data as usual will actually result in an estimator whose distribution's tails are even heavier than those of the sampling distribution.}
\corollary{}{If $\alpha_1,\alpha_2\geq 0$ with $\alpha_1+\alpha_2=1$, then $\alpha_1 Z_1+\alpha_2Z_2\sim\operatorname{Cauchy}$.}
Markowitz relates directly to this corollary. The Markowitz portfolio tries to minimize the variance of $\alpha_1 r_1+\alpha_2 r_2$, but if the returns are Cauchy, then this is a useless minimization attempt. That by Natesh, this is also true when $Z_1$ and $Z_2$ are not independent.

\section{Bootstrapping}
Given samples $X_1,\cdots,X_n$, suppose we are trying to analyze the average value. We can take the average directly of these samples to get $\overline X$. If we want more information about the distribution of the sample means, we can resample. We take $n$ samples from replacement $X_1^{(1)},\cdots,X_m^{(1)}$ which we can then use to estimate resampled mean $\overline X^{(1)}$. The most common choice is $m=n$. Clearly we can repeat this procedure as many times as we want, and this is nice because we can essentially do more analysis without actually having to gather more data, which can be costly, inconvenient, or impossible. However, note that the bootstrap only estimates the true distribution and variance of the data since it is essentially assuming that the entire population looks like our sample.

Suppose we are tracking the price of a stock $P_t$ with $P_{t+1}=P_t + r_t$. Next class we will explore various models for this. If we have $P_t$ and past data $\{P_i\}_{i=0}^{t-1}$, then we could bootstrap the previous returns in order to estimate a confidence interval for future price $P_{t+\delta}$. In particular, we sample $r_t,r_{t+1},\cdots,r_{t+\delta-1}$ from the returns $r_0,r_1,\cdots,r_{t-1}$. This procedure is nice because it makes no parametric assumptions about the data. However, this method does have some weaknesses; probably most importantly, we are assuming that future returns look like previous returns. It also has issues with dependencies between consecutive returns. One way to deal with this is by looking at serial returns, for example by looking at every three day stretch within the time period, and then sampling from that set of three day stretches.

