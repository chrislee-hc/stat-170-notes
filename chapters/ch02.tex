\chapter{1/25/2023}
\section{More on Random Walks and CLT}
Consider the random walk that starts at 0 and at each step increases or decreases by 1, each with probability 1/2. More formally, we have $S_n=\sum_{i=1}^n x_i$ where each $x_i$ is independently either $0$ or $1$ with probability 1/2 for $n\in[1,\cdots,N]$ for some $n\in\mathbb Z^+$. Note that $\Var(x_i)=1$, so by CLT, for sufficiently large $N$, $S_N$ approaches $\mathcal N(0,N)$. Thus, we can make an approximately 95\% confidence interval of $S_N$ using the two standard deviation estimation: $[-2\sqrt N, 2\sqrt N]$.

\theorem{Central Limit Theorem}{
	If $X_i$ are i.i.d. with $\mathbb E[X_i]=\mu$ and $\Var(X_i)=\sigma^{2}$, then the distribution of the sample mean $\overline X=\frac{1}{n}\sum_{i=1}^n X_i$ is approximately $\mathcal N\left(\mu,\frac{\sigma^2}{n}\right)$.
}
There's a bunch of nice things about this theorem: it's independent of the starting distribution of $X_i$, we can control the spread of the mean $\Var(\overline X)=\sigma^2/n$, the resulting distribution is normal. The normal distribution itself also has many nice properties: symmetry, maximizing entropy (among distributions with same mean and variance), etc.

\section{Generalizing the Normal Distribution to Higher Dimensions}
We can define two variables $X$ and $Y$ which are given by a bivariate distribution: $$ \begin{pmatrix} X \\ Y \end{pmatrix} \sim \mathcal N\left(\begin{pmatrix} \mu_X \\ \mu_Y \end{pmatrix},\underbrace{\begin{pmatrix} \sigma_X^2 & \sigma_{XY} \\ \sigma_{XY} & \sigma_Y^2 \end{pmatrix}}_{\text{covariance matrix }\mathbf\Sigma}\right). $$ Since the covariance matrix is $\mathbf\Sigma$, its eigenvalues are all real. It is also positive definite -- its eigenvalues are all positive. Also, $\Cov(X,Y)\leq\sqrt{\Var(X)\Var(Y)}$, and $$ \Corr(X,Y)=\frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}. $$ Note that correlation is unitless and is between -1 and 1 inclusive. The analog of the standard normal distribution in two dimensions is $$ \begin{pmatrix} X \\ Y \end{pmatrix}\sim\mathcal N\left(\begin{pmatrix} 0 \\ 0 \end{pmatrix},\begin{pmatrix} 1 & \rho \\ \rho & 1 \end{pmatrix}\right) $$ where the correlation $\rho\in[-1,1]$. Note that since the variances are both 1, correlation and covariance are equal here. Note that $\rho=0$ implies that $X$ and $Y$ are independent. This is not trivial -- correlation of 0 does not always imply independence. Furthermore, if we generate $X$ and $Y$ using this bivariate distribution, then the eigenvectors of $\mathbf\Sigma$ are the axes of symmetry of the plot of the joint distribution of $X$ and $Y$, and this is the key idea behind PCA. One other nice property is that if $$ \begin{pmatrix} X \\ Y \end{pmatrix}\sim\mathcal N\left(\begin{pmatrix} 0 \\ 0 \end{pmatrix}, \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}\right), $$ then $$ \mathbf\Sigma^{1/2} \begin{pmatrix} X \\ Y \end{pmatrix}\sim\mathcal N\left(\begin{pmatrix} 0 \\ 0 \end{pmatrix}, \mathbf\Sigma\right). $$

