\documentclass[12pt]{report}

\input{preamble}
\input{macros}
\input{letterfonts}

\title{Stat 170 Spring 2023 Notes}
\author{}
\date{}

\begin{document}
\maketitle
\tableofcontents

\chapter{1/23/2023}
Random walks, completing the square (for MGF expectations)

\chapter{1/25/2023}
\section{More on Random Walks and CLT}
Consider the random walk that starts at 0 and at each step increases or decreases by 1, each with probability 1/2. More formally, we have $S_n=\sum_{i=1}^n x_i$ where each $x_i$ is independently either $0$ or $1$ with probability 1/2 for $n\in[1,\cdots,N]$ for some $n\in\mathbb Z^+$. Note that $\Var(x_i)=1$, so by CLT, for sufficiently large $N$, $S_N$ approaches $\mathcal N(0,N)$. Thus, we can make an approximately 95\% confidence interval of $S_N$ using the two standard deviation estimation: $[-2\sqrt N, 2\sqrt N]$.

\theorem{Central Limit Theorem}{
	If $X_i$ are i.i.d. with $\mathbb E[X_i]=\mu$ and $\Var(X_i)=\sigma^{2}$, then the distribution of the sample mean $\overline X=\frac{1}{n}\sum_{i=1}^n X_i$ is approximately $\mathcal N\left(\mu,\frac{\sigma^2}{n}\right)$.
}
There's a bunch of nice things about this theorem: it's independent of the starting distribution of $X_i$, we can control the spread of the mean $\Var(\overline X)=\sigma^2/n$, the resulting distribution is normal. The normal distribution itself also has many nice properties: symmetry, maximizing entropy (among distributions with same mean and variance), etc.

\section{Generalizing the Normal Distribution to Higher Dimensions}
We can define two variables $X$ and $Y$ which are given by a bivariate distribution: $$ \begin{pmatrix} X \\ Y \end{pmatrix} \sim \mathcal N\left(\begin{pmatrix} \mu_X \\ \mu_Y \end{pmatrix},\underbrace{\begin{pmatrix} \sigma_X^2 & \sigma_{XY} \\ \sigma_{XY} & \sigma_Y^2 \end{pmatrix}}_{\text{covariance matrix }\mathbf\Sigma}\right). $$ Since the covariance matrix is $\mathbf\Sigma$, its eigenvalues are all real. It is also positive definite -- its eigenvalues are all positive. Also, $\Cov(X,Y)\leq\sqrt{\Var(X)\Var(Y)}$, and $$ \Corr(X,Y)=\frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}. $$ Note that correlation is unitless and is between -1 and 1 inclusive. The analog of the standard normal distribution in two dimensions is $$ \begin{pmatrix} X \\ Y \end{pmatrix}\sim\mathcal N\left(\begin{pmatrix} 0 \\ 0 \end{pmatrix},\begin{pmatrix} 1 & \rho \\ \rho & 1 \end{pmatrix}\right) $$ where the correlation $\rho\in[-1,1]$. Note that since the variances are both 1, correlation and covariance are equal here. Note that $\rho=0$ implies that $X$ and $Y$ are independent. This is not trivial -- correlation of 0 does not always imply independence. Furthermore, if we generate $X$ and $Y$ using this bivariate distribution, then the eigenvectors of $\mathbf\Sigma$ are the axes of symmetry of the plot of the joint distribution of $X$ and $Y$, and this is the key idea behind PCA. One other nice property is that if $$ \begin{pmatrix} X \\ Y \end{pmatrix}\sim\mathcal N\left(\begin{pmatrix} 0 \\ 0 \end{pmatrix}, \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}\right), $$ then $$ \mathbf\Sigma^{1/2} \begin{pmatrix} X \\ Y \end{pmatrix}\sim\mathcal N\left(\begin{pmatrix} 0 \\ 0 \end{pmatrix}, \mathbf\Sigma\right). $$

\chapter{1/30/2023: Introduction to Market Making}
\textit{Brian Yates -- Trader at Clear Street Markets}

\section{Market Making}
Market Making Model: Check out \href{https://www.math.nyu.edu/~avellane/HighFrequencyTrading.pdf}{paper by Avellaneda and Stoikov: ``High-frequency trading in a limit order book.''}

\section{ETF Market Making}
Just do sumproducts.

\chapter{2/1/2023}
\section{Linear Regression Review}
Many packages by default do not include an intercept. Unless there's a compelling reason why you shouldn't include one, you should add it manually if necessary. Furthermore, no causalities can be drawn from a regression; it's a purely correlational thing. The generally important quantities to look at in the output of the regression: \begin{itemize}
	\item Betas: association between the predictor and response, holding all the other predictors constant
	\item $p$-values: the significance of the difference of the betas from 0
	\item $R^2$; the percent variance (unexplained by an intercept/mean-only model) explained by the model
\end{itemize} A simple regression example is in CAPM, which regresses $r_i=\alpha+\beta Z_i+\varepsilon_i$, where $r_i$ is the return of some stock on day $i$, and $Z_i$ is the return of ``the market`` (generally some index or ETF such as SPY) on day $i$.

One common measure for risk-adjusted returns is the Sharpe ratio, calculated as $S_a=\mathbb E[R-R_F]/\sigma_R$, where $R$ is the return of the asset, $R_F$ is the returns of a risk-free asset, and $\sigma_R$ is the standard deviation of the asset excess return.

\chapter{2/6/2023}
Portfolio: $$ \mathbf P_t=\begin{pmatrix} P_{t,1} \\ P_{t,2} \\ \vdots \\ P_{t,q} \end{pmatrix} $$ and weights $\bs\alpha^\top=(\alpha_{t,1},\alpha_{t,2},\cdots,\alpha_{t,q})$. The initial value of the position is $$ \Pi_t=\bs\alpha_t^\top \mathbf P_t=\sum \alpha_{t,j}P_{t,j}. $$ A long position has $\alpha_{t,j}>0$ while a short position has $\alpha_{t,j}<0$. We define $$ w_{t,j}=\frac{\alpha_{t,j}P_{t,j}}{\Pi_t}. $$ Clearly, $\sum_{j=1}^q w_{t,j}=1$ for all $t$. The total long position is $$ \text{long}_t = \sum_{j=1}^q w_{t,j}\mathbf 1[w_{t,j}>0] $$ and similarly the total short position is $$ \text{short}_t = -\sum_{j=1}^q w_{t,j}\mathbf 1[w_{t,j}<0]. $$ By construction, $\text{long}_t-\text{short}_t=1$. Assuming no change in weights, the portfolio returns are then $$ r_{t+1}^\Pi=\frac{\Pi_{t+1}-\Pi_t}{\Pi_t}. $$ We can verify that this equals $$ \sum_{j=1}^q w_{t,j} r_{t+1,j}, $$ i.e. that the arithmetic returns of the portfolio is the weighted average of the returns of the individual substituents of the portfolio.

\chapter{2/8/2023: Bonds}
\section{Bonds}
Suppose we have $P_T$ amount of capital and invest it at the risk free rate $r^F$. After one year, our capital will grow to $P_T(1+r^F)$. If we have a different risk free rate in each year, then our principal will compound to $$ P_T = P_0\prod_{t=1}^T(1+r_t^F). $$ For example, suppose that we have have an asset that will pay $Q_T=100$ at time $T$. Then the price we would pay $P_0=M_TQ_T$, where $M_T=\frac{1}{1+r^F}$ or whatever the interest rate compounds to over time. In this case, $M_T$ is essentially the discount rate. The time $T$ is called the time to maturity. Then the \textbf{yield} is the single interest rate that will give the same total compound interest the combined effects of the $r_t^F$s: $$ (1+\hat i_T)^T = \prod_{t=1}^T(1+r_t^F) = \frac{1}{M_T}. $$ If we take the log of both sides, then we get \begin{align*}
	T\log(1+\hat i_T)^T &= \sum_{t=1}^T\log(1+r_t^F) \\
	\hat i_T &= \exp\left\{\frac{1}{T}\sum_{t=1}^T\log(1+r_t^F)\right\} - 1 \\
		&= \exp\left\{\frac{1}{T}\log\left(\frac{Q_T}{P_0}\right)\right\}-1.
\end{align*} Then if we plot $\hat i_T$ vs. $T$, we get the \textbf{yield curve}.

\section{PCA}
We can transform our observed data by projecting the dataset onto the space defined by the top $m$ PCA components, which are given by the eigenvectors of the covariance matrix of the data. The key idea is so that each PC explains as much of the variance in the data as possible. Often times the first few principal components the vast majority of the variance in the data, and thus PCA can be a dimensionality reduction technique.

Notably with respect to bonds, we can look at the yields for the treasuries of various maturities to get a sense of what is happening in the treasuries market. The first principal component is mostly an average of the rates at all expiries, and is called the level. The second principal component is essentially the rates of the treasuries with long expiry minus the rates of those with short expiry; this check for an ``inversion'' of the yield curve has become an indicator of recession. This second principal component is called the slope. The third principal component roughly tells you the difference of the slope at high expiries and the slope at low expiries and is called the curvature. This third principal component explains much less of the variance than the first two PCs do. Note that this principal component analysis is totally unsupervised -- we did not provide any labels for the data; the PCs simply give the vectors that explain the most variance in the data itself.

\chapter{2/13/2023: Martingales}
\section{Some Background}
First recall Adam's Law, or the Law of Total Expectation, which states that $\mathbb E[\mathbb E[X|Y]] = \mathbb E[X]$. Martingales are based on this concept in a slightly different language.

We will be thinking about conditional expectation using the framework of \textit{information}. Define $\mathcal F_Y$ to be the information contained in $Y$. Then we can write $\mathbb E[X|\mathcal F_X] = X$; this is essentially the more formal (``mathematically correct``) way to write $\mathbb E[X|X] = X$. Then we can rewrite Adam's Law as $$ \mathbb E[\mathbb E[X|\mathcal F_Y]] = \mathbb E[X]. $$ Similar to the logic behind Adam's Law, we can also have $\mathbb E[\mathbb E[X|Y,Z]|Z] = \mathbb E[X|Z]$. Written in the information framework, we have $$\mathbb E[\mathbb E[X|\mathcal F_Y]|\mathcal F_Z]=\mathbb E[X|\mathcal F_Z]$$ given that $\mathcal F_Y\supseteq\mathcal F_Z$, meaning that $Y$ contains at least as much information as $Z$ (for example, if $Y=X^2$, then $\mathcal F_X\supseteq\mathcal F_Y$).

Also recall Markov Chains, which have the Markov property $$ \mathbb P[X_{t+1}|X_t,X_{t-1},\cdots,X_1] = \mathbb P[X_{t+1}|X_t]. $$ 

\section{Martingale Definition}
\dfn{Martingale}{
	$(M_t,\mathcal F_t)$ is a \textbf{martingale} if 
	\begin{itemize}
		\item $\mathcal F_t\supseteq\mathcal F_{t-1}$ where $F_t=\operatorname{Inf}(M_1,\cdots,M_t)$ is the information contained in the previous $M_i$ values; this means that we have at least as much information at time $t$ as we do at time $t-1$
		\item $\mathbb E[M_t|\mathcal F_{t-1}]=M_{t-1}$ for all $t$
		\item $\mathbb E[|M_T|]<\infty$
	\end{itemize}
}
Concerning the last point, variance can be infinite, but infinite expected values would break everything. Also note that the key difference from Markov Chains is that Martingales make no claims about distributions and instead only care about means.

\ex{}{
Suppose that $X_i$ are i.i.d. with $\mathbb E[X_i]=0$, and define \begin{align*}
	M_0 &= 0 \\
	M_n &= \sum_{k=1}^n X_k \\
	\mathcal F_n &= \operatorname{Inf}(X_1,\cdots,X_n).
\end{align*} Then $(M_n,\mathcal F_n)$ is a martingale since we have $$ \mathbb E[M_n|\mathcal F_{n-1}] = \mathbb E[X_n + M_{n-1}|\mathcal F_{n-1}] = \mathbb E[X_n|\mathcal F_{n-1}] + \mathbb E[M_{n-1}|F_{n-1}] = \mathbb E[X_n] + M_{n-1} = M_{n-1}. $$}

\theorem{}{
	If $m<n$, then $\mathbb E[M_n|\mathcal F_m]=M_m$.
}
\begin{proof}
	By the generalization of Adam's law from before, \begin{align*}
		\mathbb E[M_n|\mathcal F_{n-2}] &= \mathbb E[\mathbb E[M_n|\mathcal F_{n-1}]|\mathcal F_{n-2}] \\
										&= \mathbb E[M_{n-1}|\mathcal F_{n-2}] \\
										&= M_{n-2}.
	\end{align*} Then it's pretty clear we can formalize this iterative argument using induction.
\end{proof} 
\corollary{}{
	$\mathbb E[M_n] = \mathbb E[M_m]$.
}
\proof{
	Take unconditional expectations on both sides of the previous theorem and use Adam's Law.
}

Consider another example of a security that pays off $M_T$ at end time $T$. Then at times $t<T$, our expected end payoff is $P_t=\mathbb E[M_T|\mathcal F_t]$. We claim that $(P_t,\mathcal F_t)$ is a Martingale. The proof is by definition and generalized Adam's law: $$ P_t = \mathbb E[M_T|\mathcal F_t] = \mathbb E[\mathbb E[M_T|\mathcal F_{t+1}]|\mathcal F_t] = \mathbb E[P_{t+1}|\mathcal F_t]. $$ 

Notably, for some Martingale $(P_t,\mathcal F_t)$, then the expectation of the returns is 0: \begin{align*}
\mathbb E \left[ \frac{P_{t+1}-P_t}{P_t}\bigg\rvert\mathcal F_t \right]  &= \mathbb E \left[ \frac{P_{t+1}}{P_t}\bigg\rvert\mathcal F_t \right] - 1 \\
																		 &= \frac{1}{P_t}\mathbb E[P_{t+1}|\mathcal F_t] - 1 \\
																		 &= \frac{1}{P_t}\cdot P_t - 1 \\
																		 &= 0.
\end{align*} 

\dfn{Martingale Difference}{Let $(Y_t,\mathcal F_t)$ be a Martingale. Then $\varepsilon_t=Y_t-Y_{t-1}$ is a \textbf{Martingale Difference}, sometimes denoted as $\varepsilon_t = \Delta Y_t$. This is essentially the discrete first derivative of the sequence.}

We can also rewrite this definition to get \begin{align*}
	Y_{t+1} &= Y_t + \varepsilon_{t+1} \\ 
			&= Y_{t-1} + \varepsilon_t + \varepsilon_{t+1} \\
			&= \cdots
\end{align*} 

The core properties of a Martingale and its differences are: \begin{enumerate}
	\item $\mathbb E[Y_{t+s}|\mathcal F_t] = Y_t\qquad\forall s\geq 0$
	\item $\mathbb E[\varepsilon_{t+s}|\mathcal F_t] = 0\qquad\forall s\geq 0$
	\item $\operatorname{Corr}(\varepsilon_{t+i},\varepsilon_{t+j}|\mathcal F_t)=0\qquad i,j\geq 0, i\neq j$
	\item $\Var(Y_{t+s}-Y_t|\mathcal F_t) = \sum_{j=1}^{s} \Var(\varepsilon_{t+j}|\mathcal F_t). $
\end{enumerate} Note that conditions 2 and 3 are useful when doing a regression (probably some sort of autoregressive model predicting $Y_{t+1}=\beta Y_t+\varepsilon_{t+1}$. All four of these properties are also true unconditionally: \begin{enumerate}
	\item $\mathbb E[Y_{t+s}] = \mathbb E[Y_t]$
	\item $\mathbb E[\varepsilon_{t+s}] = 0$
	\item $\operatorname{Corr}(\varepsilon_{t+i},\varepsilon_{t+j}) = 0$.
	\item $\Var(Y_{t+s}-Y_t) = \sum_{j=1}^{s} \Var(\varepsilon_{t+j}). $
\end{enumerate} 
Also to see the relationship between some of these properties, we can use property 3 to see that \begin{align*}
	\Var(Y_{t+s}-Y_t|\mathcal F_t) &= \sum_{j=1}^{s} \sum_{i=1}^{s} \Cov(\varepsilon_{t+j},\varepsilon_{t+i}|\mathcal F_t)   \\
								   &= \sum_{i=1}^{s} \Var(\varepsilon_{t+j}|\mathcal F_t) + 2 \sum_{i\neq j}^{} \Cov(\varepsilon_{t+i},\varepsilon_{t+j}|\mathcal F_t) \\
								   &= \sum_{j=1}^{s} \Var(\varepsilon_{t+j}|\mathcal F_t), 
\end{align*} which gives property 4.

\chapter{2/15/2023: Martingales Continued}
\section{Computations}
We will first prove the third property of Martingales from last class:
\clm{}{}{For $i\neq j$, we have $\Cov(\varepsilon_{t+j},\varepsilon_{t+i}|\mathcal F_t)=0$.}
\pf{Utilizing the definition of covariance, we can write $$ \Cov(\varepsilon_{t+i},\varepsilon_{t+j}|\mathcal F_t) = \mathbb E[\varepsilon_{t+i}\varepsilon_{t+j}|\mathcal F_t] - \mathbb E[\varepsilon_{t+i}|\mathcal F_t]\mathbb E[\varepsilon_{t+j}|\mathcal F_t]. $$ By property 2 from last class, we have $\mathbb E[\varepsilon_{t+i}|\mathcal F_t]\mathbb E[\varepsilon_{t+j}|\mathcal F_t]=0$. Then without loss of generality assume that $i<j$. By Generalized Adam's Law, we then get $$ \mathbb E[\varepsilon_{t+i}\varepsilon_{t+j}|\mathcal F_t] = \mathbb E[\mathbb E[\varepsilon_{t+i}\varepsilon_{t+j}|\mathcal F_{t+i}]\mathcal F_t] = \mathbb E[\varepsilon_{t+i}\mathbb E[\varepsilon_{t+j}|\mathcal F_{t+i}]|\mathcal F_t]. $$ Again by property 2, we get $\mathbb E[\varepsilon_{t+j}|\mathcal F_{t+i}]=0$, which means that our original covariance is 0.}

Next, suppose we have a self-financing portfolio of stocks over time with prices $\{\mathbf P_t\}_{t=1}^T$ and histories $\{\mathcal F_t\}_{t=1}^T$, and suppose at time $t$ we have weights $\bs\alpha_t$ in each of the stocks. Because this portfolio is self-financing, we do not want any cash to flow into or out of the portfolio and, assuming that we can instantaneously reweight our portfolio at each time step, this means that $\bs\alpha_{t-1}^\top\mathbf P_t = \bs\alpha_t^\top\mathbf P$.

Let $\Pi_t=\bs\alpha_t^\top\mathbf P_t$ be the total value of the portfolio at time $t$. Then the profit gained between days $t$ and $t+1$ is \begin{align*}
	\Pi_{t+1}-\Pi_t &= \bs\alpha_t^\top\mathbf P_{t+1} - \bs\alpha_{t-1}^\top\mathbf P_t \\
					&= \bs\alpha_t^\top\mathbf P_{t+1} - \bs\alpha_t^\top\mathbf P_t \\
					&= \bs\alpha_t^\top(\mathbf P_{t+1}-\mathbf P_t).
\end{align*} This is relatively obvious, it just says your total profit is the weighted average of the profits from your portfolio.

\clm{}{}{If $(\mathbf P_t,\mathcal F_t)$ is a Martingale, then $(\Pi_t,\mathcal F_t)$ is also a Martingale.}
\pf{This follows fairly directly from above and from the properties of Martingale differences: we first see that the expected difference between consecutive $\Pi_t$ given the relevant information is 0: $$ \mathbb E[\Pi_{t+1}-\Pi_t|\mathcal F_t] = \mathbb E[\bs\alpha_t^\top(\mathbf P_{t+1}-\mathbf P_t)|\mathcal F_t] = \bs\alpha_t^\top\mathbb E[\mathbf P_{t+1}-\mathbf P_t|\mathcal F_t] = 0. $$ This then implies that $(\Pi_t,\mathcal F_t)$ satisfies the key condition of being a Martingale: $$ \mathbb E[\Pi_{t+1}|\mathcal F_t] = \mathbb E[\Pi_t|\mathcal F_t] = \Pi_t. $$ }
This shows us that we can construct new Martingales from old ones; in particular, in this example we are creating the new Martingale $(\Pi_t,\mathcal F_t)$ from the Martingale differences of the original Martingale $(\mathbf P_t,\mathcal F_t)$. More generally, if we have Martingale $(M_t,\mathcal F_t)$ and random variables $\{A_t\}_{t=1}^n$ with $\mathbb E[A_t|\mathcal F_t]=A_t$ (i.e. $\mathcal F_t$ encodes all possible information about $A_t$? I think), then if we define $$ Z_n=\sum_{t=1}^{n} A_t(M_t-M_{k-1}), $$ then $(Z_t,\mathcal F_t)$ is a Martingale. If we have continuous time periods, then this becomes the integral $\int A_t\mathrm dM_t$, which is the basis of stochastic calculus.

\section{Doob's Optional Stopping Theorem}
\subsection{Definitions and Theorem}
\dfn{Martingale Stopping Times}{The \textbf{stopping time} $\tau$ of martingale $(M_n,\mathcal F_n)$ is a random variable whose value is interpreted as the time at which a given stochastic process exhibits a specific behavior of interest. In particular, we always need to be able to answer whether or not time $\tau$ has already occurred by time $t$ given information $\mathcal F_t$.}
\ex{Stopping Times and Not Stopping Time}{
Here are a few examples to emphasize what defines a stopping time and examples of random variables that do not satisfy this condition.
\begin{itemize}
	\item If $\tau$ is the first time that our portfolio is down \$5, then it is a stopping time.
	\item If $\tau$ is the time at which a stock reaches its daily minimum, then it is not a stopping time, because we can never know if it happened or not without looking into the future (or until the day ends).
\end{itemize}
}
\theorem{Doob's Optional Stopping Theorem}{If we have stopping time $\tau$ with $\mathbb E[\tau]<\infty$ and some bound $B$ such that $|M_n-M_{n-1}|\leq B$, then $\mathbb E[M_\tau] = \mathbb E[M_0]$.}

\subsection{Examples}
\noindent To show how powerful this theorem is, we'll look at a few nice problems.
\qs{Gambler's Ruin}{Let $X_i\sim\operatorname{Bern}(1/2)$ be a sequence of random variables and let $M_n=\sum_{i=1}^n X_i$ so that $M_n$ when paired with the relevant information is a Martingale. Given some $A,B\in\mathbb Z^+$, what is the probability that the sequence $\{M_t\}_{t=1}^\infty$ reaches the value $A$ before it reaches $-B$?}
\begin{solution}
	Let $\tau$ be the first time the sequence hits either $A$ or $-B$. Note that this is a stopping time since we can always verify whether this has happened yet and since the bounding conditions holds. Then by Doob (it's straightforward to verify that the necessary conditions are satisfied), $\mathbb E[M_\tau] = 0$, which tells us that the probability of $M_\tau$ being $A$ (i.e. that the sequence hits $A$ first) is $\frac{B}{A+B}$.
\end{solution}
	Note that if our stopping condition was only the first time we hit $A$ (and not bounding the other side), then Doob's Theorem would not hold because $\mathbb E[\tau]$ would not be finite.

\qs{oh no chris is drunk again}{
	Suppose we have a drunk monkey (named Chris) with a typewriter jamming away at the keys. What's the expected time (number of key presses) it will take him to type \textit{ABRACADABRA}? (Assume the monkey only types capital letters.)
}

\begin{solution}
	Suppose a casino takes fair bets on the letters the monkey types such that if someone bet \$1 that the next letter the monkey will type is an A, then they will get \$26 if the monkey does indeed type an A with its next letter and nothing if not. The casino will stop taking bets once the monkey has successfully typed \textit{ABRACADABRA}. \\[0.1in]
	Suppose further that at first there is one bettor and that with each key the monkey presses, another bettor shows up. Each of these bettors have the exact same behavior: \begin{itemize}
		\item The bettor shows up to the casino with \$1.
		\item The bettor leaves the casino once either they lose all their money or the 
		\item The bettor bets all their money on the first letter that they have not yet bet on.
	\end{itemize}
	Since each bettor bets exactly \$1 and never has any other cash flows (since they will subsequently re-bet their entire amount on the next letter) unless the monkey has typed the magic word, the casino will have \$$\tau$ after the monkey has typed $\tau$ letters before having to pay off any bets. Furthermore, since each of the bets is fair (i.e. has expected value 0), the amount of money $M_t$ the casino has at each time step $t$ (before the bettors re-bet their earnings from the previous key press) is a Martingale with respect to the information in the previous time steps, and we can easily check that the conditions of Doob's OST hold. Thus, if we have stopping time $\tau$, then we have $\mathbb E[M_\tau]=\mathbb E[M_0]$. Since the casino starts with no money and only takes fair-valued bets, $\mathbb E[M_0]=0$. Furthermore, at time $\tau$ the casino will have garnered \$$\tau$ worth of payments, so if the casino has to pay off $P$ to the bettors at the end (once the monkey has typed \textit{ABRACADABRA}), then $$ \mathbb E[M_\tau] = \mathbb E[\tau-P] = \mathbb E[\tau]-P = 0 \implies \mathbb E[\tau] = P. $$ Thus, the expected stopping time is exactly the amount that the casino will have to pay the bettors in the end. When the monkey has typed the magic word, the casino will have to pay exactly four bettors: \begin{itemize}
		\item One bettor will win \$$26^{11}$ since they started betting 11 time steps previously and won all 11 bets.
		\item One bettor will win \$$26^4$ since they started betting 4 time steps previously and won all 4 bets due to the fact that the first and last four letters of \textit{ABRACADABRA} are the same.
		\item One bettor will win \$$26$ since they won the bet that the monkey would type an A, the last letter of the magic phrase.
	\end{itemize}
	Thus, the expected stopping time is $$ \mathbb E[\tau] = P = 26^{11} + 26^4 + 26. $$ 
\end{solution}

\chapter{2/22/2023: Martingales (Final?)}
\section{More Martingale Computations}
Recall the drunk monkey problem from the end of last class. We can solve similar problems such as the number of (fair) coin flips expected to flip the following series \begin{enumerate}
	\item HHH ($2^3+2^2+2^1=14$)
	\item HTH ($2^3+2^1=10$)
	\item HTT ($2^3=8$)
\end{enumerate}
\nt{
	\textit{This isn't important but I thought it was interesting; feel free to skip this note.} \\[0.1in]
	Upon first glance, it may seem fairly unintuitive that \textit{more} repetition in the desired sequence is associated with a \textit{later} expected stopping time. After all, the sequence HHH has a lot of symmetry, and any individual heads toss can show up in the stopping sequence in three different places. Shouldn't this mean that the sequence HHH is ``easiest'' to obtain and thus have the lowest expected stopping time? \\[0.1in]
	I think this logic fails because of the sequential nature with which these games are played. In particular, when we report an individual flip, that result is not in isolation; it just becomes the last result in a sequence of flips that have already been decided. Thus, if the previous flip was tails and the new flip is heads, then this heads can only be in the first position of the HHH goal sequence (if it is part of that sequence at all). Similarly, if the previous two flips were tails then heads, then flipping a heads could only put it in the second position of the goal HHH sequence (again, \textit{if} it ends up being part of that sequence at all). Thus, the symmetry that seems like it should make sequences such as HHH easier to obtain actually does not have a helpful effect at all. \\[0.1in]
	This explains why the expected stopping time for the ``symmetric'' sequences shouldn't be lower than those of the ``assymmetric'' sequences, but as we have calculated, the ``symmetric'' sequences stopping times are in fact \textit{higher} -- why is this the case? For this, consider again the scenario of the monkey typing letters A-Z, and suppose we want to find the expected number of key presses until it types a certain phrase. We will think about what happens if this phrase is ABCXYZ compared to if it is ABCABC. \\[0.1in]
	Using the same methods as before, we find that the expected number of key presses it takes the monkey to type ABCXYZ and ABCABC are $26^6$ and $26^6+26^3$, respectively. If we are waiting until the monkey types ABCXYZ, then at any given point, if the monkey has not recently typed a useful sequence, then the monkey has a $1/26$ chance of typing an X and starting off the sequence. Once the monkey has typed an X, then there are three scenarios for the next letter: \begin{itemize}
		\item The monkey types another A (with probability $1/26$), restarting the phrase ABCXYZ
		\item The monkey types a B (with probability $1/26$) so that it has now typed two of the characters in the phrase
		\item The monkey types a different letter (with probability $24/26$), essentially taking it back to the beginning in terms of progress
	\end{itemize} These three cases are the same no matter where the monkey is in typing the word. However, if we instead consider the case when we are waiting for the monkey to type ABCABC, then these three cases are the same \textit{unless} the last three letters it has typed are ABC. In this case, the first two cases are the same, and the monkey instead has a $25/26$ chance of typing a letter that destroys all its progress. Thus, since this progress-destroying probability has increased, the monkey will have to restart more often (in expectation) from this juncture, and thus the expected number of key presses it takes to type the phrase should be higher compared to the ABCXYZ case. Notably, when length of the substring repeated at the beginning and end of the key phrase is longer, the more consequential this increase in the probability of the bad scenario, and thus the greater the expected value of number of key presses.
}

\section{Importance of the Amount of Information Available}
Suppose we have $P_{t+1}=P_t+X_t+\varepsilon_{t+1}$ where $\mathbb E[X_t]=0$, $\mathbb E[\varepsilon_{t+1}]=0$, $X_t$ is independent of $P_t$, and $\varepsilon_{t+1}$ is some random noise variable independent of the other variables. Furthermore let $\mathcal F_t$ and $\mathcal G_t$ be the information generated by the sequences $\{P_k\}_{k=1}^t$ and $\{(P_k,X_k)\}_{k=1}^t$, respectively. Then, we can see that \begin{align*}
	\mathbb E[P_{t+1}|\mathcal F_t] &= \mathbb E[P_t+X_t+\varepsilon_{t+1}|\mathcal F_t] \\
									&= P_t + \mathbb E[X_t|\mathcal F_t] + 0 \\
									&= P_t.
\end{align*} Thus, $(P_t,\mathcal F_t)$ is a Martingale. However, note that $$\mathbb E[P_{t+1}|\mathcal G_t] = P_t + \mathbb E[X_t|\mathcal G_t] + 0 = P_t + X_t,$$ and thus $(P_t,\mathcal G_t)$ is not a Martingale. This is essentially insider trading -- if we have access to $\mathcal G_t\supset \mathcal F_t$ which has all the information about $X_t$, then we can predict price movement better than anyone else.

\section{More Properties of Stopping Times}
Before proving Doob's OST, we explore another important theorem about stopping times.
\theorem{Wald's Theorem}{If $X_i$ are i.i.d. with finite expectation $\mathbb E[|X_i|]<\infty$ for all $i$, and if $N$ is the stopping time also with $\mathbb E[N]<\infty$, then $$ \mathbb E\left[\sum_{n=1}^N X_n\right] = \mathbb E[N]\mathbb E[X_i]. $$} This theorem is really nice because it can apply to such a wide variety of stopping times with the only limitation being that the expectation of this stopping time must be finite.
\pf{We prove this essentially by direct computation. Letting $\mathcal F_n$ be the information contained in $\{X_n\}_{n=1}^N$: \begin{align*}
		\mathbb E\left[\sum_{n=1}^{N} X_n\right] &= \mathbb E\left[\sum_{n=1}^\infty X_n\cdot\mathbf 1_{N\geq n}\right] \\
												 &= \sum_{n=1}^\infty \mathbb E \left[ \mathbb E \left[ X_n\cdot\mathbf 1_{N\geq n} \bigg\rvert\mathcal F_{n-1} \right]  \right] \\
												 &= \sum_{n=1}^{\infty} \mathbb E \left[ \mathbf 1_{N\geq n}\mathbb E[X_n|\mathcal F_{n-1} \right] .
\end{align*} Note that the last step is valid because given $\mathcal F_{n-1}$, $\mathbf 1_{N\geq n}$ is simply a constant (since we know if the stopping condition has happened before $n$). Then, this sum becomes $$ \sum_{n=1}^{\infty} \mathbb E \left[ \mathbf 1_{N\geq n}\mathbb E[X_n|\mathcal F_{n-1} \right] = \sum_{n=1}^{\infty} \mathbb E[\mathbf 1_{N\geq n}]\cdot \mathbb E[X_n] = \sum_{n=1}^\infty \mathbb P[N\geq n]\cdot\mathbb E[X_i] = \mathbb E[N]\cdot\mathbb E[X_i] $$ 
}
\nt{To see that $\sum_{n=1}^\infty \mathbb P[N\geq n]=\mathbb E[N]$ in the last equality of the proof above:
\begin{center}
\begin{tabular}{ccccccccccc}
	& $\mathbb P[N\geq 1]$ & $=$ & $\mathbb P[N=1]$ & $+$ & $\mathbb P[N=2]$ & $+$ & $\mathbb P[N=3]$ & $+$ & $\cdots$ \\
	& $\mathbb P[N\geq 2]$ & $=$ & & & $\mathbb P[N=2]$ & $+$ & $\mathbb P[N=3]$ & $+$ & $\cdots$ \\
	& $\mathbb P[N\geq 3]$ & $=$ & & & & & $\mathbb P[N=3]$ & $+$ & $\cdots$ \\
	$+$ & $\vdots$ & & $\vdots$ & & $\vdots$ & & $\vdots$ & &  \\ \hline
	& $\sum_{n=1}^\infty \mathbb P[N\geq n]$ & $=$ & $\mathbb P[N=1]$ & $+$ & $2\cdot\mathbb P[N=2]$ & $+$ & $3\cdot\mathbb P[N=3]$ & $+$ & $\cdots$ \\
	& & $=$ & $\mathbb E[N]$ & & & & & &
\end{tabular} 
\end{center} 
}

Proving Doob's Theorem is pretty tricky, so we will go part of the way there by proving what is essentially a precursor to it.
\theorem{Doob's OST Precursor}{For any stopping time $\tau$, let $\tau'=\min(\tau,n)=\tau\wedge n$. Then, for the Martingale $\{M_n,\mathcal F_n\}$, $$\mathbb E[M_{\tau\wedge n}] = \mathbb E[M_0].$$ }

Note that Doob's OST essentially comes by taking the limit as $n$ goes to $\infty$. This isn't super straightforward (can't always switch limit and expectation), but it works out nicely in this case.

To prove this theorem, we will make use of the following Lemma:
\lemma{}{
Without loss of generality, let $M_0=0$. Then, if $$Z_n = \sum_{k=1}^{n} A_k(M_k-M_{k-1}), $$ then $(Z_n,\mathcal F_n)$ is a Martingale.
}

This lemma will be proved in the homework (presumably PSet 3). We now use this lemma to prove the precursor to Doob's OST.
\pf{Note that \begin{align*}
	M_{\tau\wedge n} &= M_\tau\cdot\mathbf 1_{\tau\leq n-1} + M_n\cdot\mathbf 1_{\tau\geq n} \\
					 &= M_0 + \sum_{k=1}^n A_k(M_k-M_{k-1})
\end{align*} where $A_k=\mathbf 1_{\tau\geq k}$. The last equality can be seen through some simple expansion and telescoping. Then the sum in the last expression is a Martingale that initially has an unconditional expected value of 0, and thus taking unconditional expectations of both sides completes the proof. (Note that the $M_0$ was not on the board when Natesh wrote it, but I'm pretty sure we need it; I think it was just assumed to be 0 as in the lemma.)
}

\end{document}
